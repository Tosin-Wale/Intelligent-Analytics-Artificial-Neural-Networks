{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036831d0-1a2c-4509-aedc-7a30f9f53162",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SINGLE - LAYER PERCEPTRON \n",
    "## Oluwatosin Adewale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c79b1-1d28-490d-926f-74961b4f3c02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SINGLE LAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d374648-d748-481e-a242-a92b32f9dc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SingleLayerPerceptron:\n",
    "    def __init__(self, config):\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.max_iterations = min(config['n_iterations'], 1000)  \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.error_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_outputs = y.shape[1] if len(y.shape) > 1 else 1\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(n_features, n_outputs) * 0.01\n",
    "        self.bias = np.zeros((1, n_outputs))\n",
    "\n",
    "        iteration = 0  \n",
    "        total_error = float('inf')  \n",
    "\n",
    "        # Training process using while loop\n",
    "        while iteration < self.max_iterations and total_error > 0:\n",
    "            total_error = 0  \n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self.activation_function(linear_output)\n",
    "\n",
    "                error = y[idx] - y_predicted\n",
    "                total_error += np.sum(np.abs(error))\n",
    "\n",
    "                update = self.learning_rate * error\n",
    "                self.weights += x_i[:, np.newaxis] * update\n",
    "                self.bias += update\n",
    "\n",
    "            self.error_history.append(total_error)\n",
    "            if iteration % 10 == 0 or total_error == 0:\n",
    "                print(f\"Iteration {iteration}: Total Error = {total_error}\")\n",
    "\n",
    "            if total_error == 0:\n",
    "                print(\"Training stopped: No errors.\")\n",
    "                break\n",
    "\n",
    "            iteration += 1  \n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation_function(linear_output)\n",
    "\n",
    "    def activation_function(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def plot_error(self):\n",
    "        plt.plot(self.error_history)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Total Error')\n",
    "        plt.title('Training Error Progress')\n",
    "        plt.show()\n",
    "\n",
    "def load_config_from_csv(file_path):\n",
    "    config_df = pd.read_csv(file_path, header=None)\n",
    "    config_df.columns = ['key', 'value'] \n",
    "    \n",
    "    config_dict = pd.Series(config_df['value'].values, index=config_df['key']).to_dict()\n",
    "    \n",
    "    # Converting numeric values from strings to appropriate types\n",
    "    config_dict['learning_rate'] = float(config_dict['learning_rate'])\n",
    "    config_dict['n_iterations'] = int(config_dict['n_iterations'])\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "def load_data(path, label_column):\n",
    "    data = pd.read_csv(path)\n",
    "    y = data[[label_column]]\n",
    "    X = data.drop(label_column, axis=1)\n",
    "    return X.values, y.values.ravel() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec81eaf-4642-4674-ad42-a15739ee6e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "if __name__ == \"__main__\":\n",
    "    config_file = r'C:\\Users\\tosin\\Downloads\\SLPerceptron_configfile.csv'  # Ensure this is the correct path to your config file\n",
    "    config = load_config_from_csv(config_file)\n",
    "\n",
    "    X_train, y_train = load_data(config['training_data_path'], config['label_column'])\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    perceptron = SingleLayerPerceptron(config)\n",
    "    perceptron.fit(X_train, y_train.reshape(-1, 1))  # Reshape y_train for consistency\n",
    "\n",
    "    predictions = perceptron.predict(X_train)\n",
    "    print(\"Predictions:\", predictions.ravel())\n",
    "\n",
    "    perceptron.plot_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f028d40-6337-4e43-a6a0-4daa208bc591",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes for the Config file\n",
    "Please use a .csv configuration file that follows this pattern\n",
    "learning_rate\t0.01\n",
    "n_iterations\t100\n",
    "training_data_path\tpath_to_training_data.csv\n",
    "label_column\tLabel\n",
    "connectivity\tfull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7086e5d-e9d4-4e1a-955c-755acf004e7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Observations from the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8dc35-aba0-45fa-9065-23afc0bd5e16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "I tested this perceptron with a variety of learning rates (0.001, 0.1, 001) and epochs(Fromm 10,000 to 1,000 and finally 100) and got similar results for each run. Essentially, the perceptron's error rates fluctuated through a series of numbers between 6 and 12 regardless of how many epochs or what the learning rate was. Of all the Iterations(Epochs) ran, the lowest error after an epoch was 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e72e31-942e-4512-9004-a6c3d1c972b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PERCEPTRON LIMITER FUNCTIONALITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb33f2a-e438-481a-b766-04768bb802ae",
   "metadata": {},
   "source": [
    "To justify the argument that a single-layer perceptron can only solve problems with existing linearly separable patterns irrespective of the transfer function being a hard limiter or a soft limiter like the sigmoidal nonlinear element, I have considered the following. \n",
    "\n",
    "In the foundational architecture of a perceptron, it is intrinsically designed to address problems that are linearly separable. By its inherent configuration, a perceptron can generate solutions only if it can define categories effectively through a linear demarcation, which translates to binary (yes/no, or 0/1, or True/False) decision-making based on a certain threshold. While employing a hard limiter in a perceptron unequivocally results in definitive yes/no outcomes, the integration of a sigmoidal nonlinear element introduces a spectrum of probabilistic responses, akin to \"maybes.\" However, these gradations are ultimately variants of binary decisions. \n",
    "\n",
    "Irrespective of the degree of ambiguity that a sigmoidal function might permit, the operational principle of the perceptron remains unaltered: it endeavors to classify input by employing a singular, linear boundary. This methodology inherently categorizes decisions into distinct binary categories, represented as either side of the linear boundary. Thus, the perceptron lacks the capacity to acknowledge or classify inputs that would metaphorically fall 'on the line'. \n",
    "\n",
    "Furthermore, in an example scenario such as distinguishing between warm and cool colors, the perceptron employs a predetermined threshold to categorize inputs. This approach precludes the possibility of recognizing 'neutral' or intermediary categories, thereby reinforcing its binary classification nature. Consequently, problems that deviate from linear separability cannot be effectively resolved within this model, necessitating alternative computational frameworks for their resolution. \n",
    "\n",
    "In summary, a hard limiting transfer function makes a strong division in its category as either yes/no, or even 0/1, and while a sigmoidal element may endow the perceptron with a nuanced range of binary-like responses, it remains constrained within the yes/no paradigm. The perceptron's fundamental limitation as a linear classifier precludes it from addressing complex, non-linearly separable problems. Adding a nonlinear element does nothing to allow the perceptron to solve problems with non-linearly separable patterns. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740a201-757f-4c0f-9306-fb9adfd7bec5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## XOR PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90422e61-9e03-46fe-b6b4-91c286b4a5c7",
   "metadata": {},
   "source": [
    "A perceptron is a computational model that simulates a decision-making process like a neuron in the brain. It is adept at performing binary classifications, which can be extended to model simple logic operations such as AND, OR, and COMPLEMENT. However, its architecture imposes inherent limitations, notably in implementing the XOR (Exclusive OR) function. Below is a description of the implementations of the AND, OR, and COMPLEMENT functions as well as a perceptron's limitations regarding the XOR function. \n",
    "\n",
    "1. AND Function: The AND function operates by stipulating a conjunctive condition where both inputs, denoted as A and B, must simultaneously satisfy a specific criterion to yield a positive outcome ('True'). Conversely, if either A or B fails to meet the requisite condition, the result defaults to a negative outcome (‘False'). This binary classification is emblematic of linear separability, where a single decision boundary (a linear demarcation) can effectively segregate the outcomes. Put more technically, the perceptron can effectively model the AND logic function by assigning appropriate weights to its inputs. In this configuration, the perceptron outputs a '1' (True) only when all inputs - let's call them A and B are '1' (True), making any other option '0'(False) reflecting the logical AND requirement. This is accomplished by setting the weights and threshold such that the cumulative weighted sum of the inputs surpasses the threshold only when both inputs are true. \n",
    "\n",
    "2. OR Function: The OR function also epitomizes binary classification by providing a positive outcome ('1') only when either input condition (either A or B) is met, and a negative outcome ('0') when neither condition is met. Like the AND function, the perceptron models the OR function by adjusting its weights and threshold to ensure that the output is '1' if at least one input is '1'. This setup allows for a binary classification that mirrors the OR operation, where the presence of at least one true input leads to a true (‘1’) output. \n",
    "\n",
    "3. COMPLEMENT Function: The COMPLEMENT (NOT) function is an operation that inverts the value of a binary output. That is, it gives whatever value is not true as the positive outcome and vice versa. This function is achieved by inversely weighting the input and properly setting the threshold. For instance, if the input is '1' (true), the perceptron outputs '0' (false), and vice versa. This inversion embodies the logical NOT operation, demonstrating the perceptron's versatility in simulating basic unary logic operations. \n",
    "\n",
    "4. XOR (Exclusive OR) Function Limitation: The perceptron's binary decision-making framework is based on linear separability, which works quite well for the AND, OR, and COMPLEMENT functions. However, the XOR function presents a unique challenge that requires the model to differentiate inputs based on a criterion that is inherently non-linear. The XOR operation necessitates an output of '1' only when the inputs are different (That is, one input is true and the other is false), a scenario that cannot be resolved with a single linear boundary. This limitation stems from the perceptron's inbuilt linear decision-making mechanism, which is incapable of segmenting the input space to accurately reflect the XOR function's non-linear criteria. Essentially, while the perceptron is successfully able to execute binary logic operations that conform to linear separability principles, it encounters a problem with the XOR function due to its reliance on a linear decision-making model. This limitation in the perceptron's base architecture is precisely what makes it unable to process patterns requiring non-linear decision boundaries for accurate classification, more specifically, situations like the XOR function. \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
